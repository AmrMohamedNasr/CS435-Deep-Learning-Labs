# -*- coding: utf-8 -*-
"""assignment1_housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M-d-bK4g8Yd9ACE01M5ReRssNmqNVN5o

# Housing linear regression
### Hyperparameters of the linear regression model:
* Since it is linear regression, the network is already defined as a 1 neuron combining all the features of the input in a linear way so layer number and number of units are not hyperparameters in this specific problem.
* Regularization type, and term is a hyper parameter to be tuned(We will find that we need to use regularization due to overfitting).
* The type of optimizer and its parameters.
* Learning rate (We will need to tune it so we reach convergence).
* Loss function.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib

import matplotlib.pyplot as plt
from scipy.stats import skew
from scipy.stats.stats import pearsonr
from keras.layers import Dense
from keras.models import Sequential
from keras.regularizers import l1
from keras.optimizers import Adam,SGD
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# %config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook
# %matplotlib inline

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],
                      test.loc[:,'MSSubClass':'SaleCondition']))

train["SalePrice"] = np.log1p(train["SalePrice"])
numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index
skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness
skewed_feats = skewed_feats[skewed_feats > 0.75]
skewed_feats = skewed_feats.index

all_data[skewed_feats] = np.log1p(all_data[skewed_feats])

all_data = pd.get_dummies(all_data)

all_data = all_data.fillna(all_data.mean())

X_train = all_data[:train.shape[0]]
X_test = all_data[train.shape[0]:]
y = train.SalePrice

X_train = StandardScaler().fit_transform(X_train)

X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)

def test_model(loss, opt, reg, X_tr, y_tr, X_val, y_val):  
  #START CODE HERE
  def Model(input_shape, reg=None):
    model = Sequential()
    model.add(Dense(1, input_shape = input_shape, kernel_regularizer = reg))
    return model;
  model = Model(X_tr.shape[1:], reg)
  #END CODE HERE
  model.compile(loss = loss, optimizer=opt)
  model.summary()
  hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 150, verbose = 0)
  print(model.predict(X_test))
  pd.Series(model.predict(X_test)[:,0]).hist()
  plt.show()
  history = hist
  # Plot training & validation loss values
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  print('Last validation loss : ', history.history['val_loss'][-1], ' | last training loss : ', history.history['loss'][-1])
  return model

"""---
# Different Configuration Results

### Default linear regression without regularization and no artificial variables with Adam optimizer with default values with Mean Square Error (MSE) as loss
"""

models = []
models.append(test_model('mse', 'adam', None, X_tr, y_tr, X_val, y_val))

"""We can notice overfitting  since training loss is reducing with each epoch yet the validation loss is increasing. Adding regularization should help.

### Default linear regression without regularization and no artificial variables with Adam optimizer with default values with Mean Absolute Error (MAE) as loss
"""

models.append(test_model('mae', 'adam', None, X_tr, y_tr, X_val, y_val))

"""### Default linear regression without regularization and no artificial variables with Adam optimizer with default values with Mean Absolute Percentage Error(mape) as loss"""

models.append(test_model('mean_absolute_percentage_error', 'adam', None, X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(0.2) and no artificial variables with Adam optimizer with default values with mse as loss"""

models.append(test_model('mse', 'adam', l1(0.2), X_tr, y_tr, X_val, y_val))

"""We can notice improvement but not enough, we need to increase the regularization factor.

### Linear regression with L1 regularization(0.5) and no artificial variables with Adam optimizer with default values with mse as loss
"""

models.append(test_model('mse', 'adam', l1(0.5), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with Adam optimizer with default values with mse as loss"""

models.append(test_model('mse', 'adam', l1(1), X_tr, y_tr, X_val, y_val))

"""No overfitting, we reach our best performance, but we still haven’t reached the convergence point, raising our learning rate should do the trick.

### Linear regression with L1 regularization(1) and no artificial variables with Adam optimizer with default values with mae as loss
"""

models.append(test_model('mae', 'adam', l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with Adam optimizer with default values with mape as loss"""

models.append(test_model('mean_absolute_percentage_error', 'adam', l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with SGD optimizer with default values(learning rate of 0.01 as opposed to adam default learning rate of 0.001) with mse as loss"""

models.append(test_model('mse', 'sgd', l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with RmsProp optimizer with 0.001 learning rate with mse as loss"""

models.append(test_model('mse', 'RMSprop', l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with Adam optimizer with 0.005 learning rate with mse as loss"""

models.append(test_model('mse', Adam(lr = 0.005), l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(1) and no artificial variables with SGD optimizer with 0.001 learning rate with mse as loss"""

models.append(test_model('mse', SGD(lr=0.001), l1(1), X_tr, y_tr, X_val, y_val))

"""### Linear regression with L1 regularization(0.1) and no artificial variables with SGD optimizer with 0.001 learning rate with mse as loss"""

models.append(test_model('mse', SGD(lr=0.001), l1(0.1), X_tr, y_tr, X_val, y_val))

"""---
# Conclusion
* Different losses didn’t do that much effect, but we can notice that mean absolute error helps prevents massive overfitting to some extent.
* Regularization is a must to achieve good performances on the validation set due to the model having high variance when trained without it.
* Different optimizers would affect when the model converges, however it is more important to tune their learning rate because as noticed, If tuned correctly, they will give similar convergence, however for our problem SGD was the best optimizer.
---
"""

best_model = models[-1]

