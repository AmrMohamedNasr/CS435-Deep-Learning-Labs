# -*- coding: utf-8 -*-
"""Mnist_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9TQmKbUT5Kf2mJDA7aJdOZoz06G_YKZ

# Multinominal Classification
## Mnist dataset classification
Classify 28x28 bit image as a digit from 0-9
"""

import tensorflow as tf
tf.enable_eager_execution()
import matplotlib.pyplot as plt
import numpy as np
import random
from progressbar import progressbar
from keras.layers import Dense
from sklearn.model_selection import train_test_split
import time

"""## Feed-Forward Neural Network Architecture"""

def build_fc_model(activation):
    fc_model = tf.keras.Sequential([
        tf.keras.layers.Flatten(),
        # TODO: Define the rest of the model.
        tf.keras.layers.Dense(128, input_shape=(784,), activation=activation),
        tf.keras.layers.Dense(10, activation='softmax')

    ])
    return fc_model

"""## Convolutional Neural Network Architecture"""

def build_cnn_model(activation):
    cnn_model = tf.keras.Sequential([
       # TODO: Define the model.
       tf.keras.layers.Conv2D(24, 3, activation = activation, input_shape=(28,28,1,)),
       tf.keras.layers.MaxPooling2D(),
       tf.keras.layers.Conv2D(36, 3, activation = activation),
       tf.keras.layers.MaxPooling2D(),
       tf.keras.layers.Flatten(),
       tf.keras.layers.Dense(784, activation=activation),
       tf.keras.layers.Dense(128, activation=activation),
       tf.keras.layers.Dense(10, activation='softmax')
    ])
    return cnn_model

"""## Experiment code to be able to run different configurations"""

def run_experiment(build_model, activation, optimizer, loss, X, y, verbose = 1):
  model = build_model(activation)
  BATCH_SIZE = 64
  EPOCHS = 5
  train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state=0)
  
  
  # TODO compile and fit the model with the appropriate parameters.
  model.compile(loss=loss, metrics=['categorical_accuracy'], optimizer=optimizer)
  
  start = time.time()
  history = model.fit(train_X, train_y, verbose=verbose, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data = (val_X, val_y) )
  end = time.time()
  
  model.summary()
  
  # Plot training & validation accuracy values
  plt.plot(history.history['categorical_accuracy'])
  plt.plot(history.history['val_categorical_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()

  # Plot training & validation loss values
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
  print('Last validation loss : ', history.history['val_loss'][-1], ' | last training loss : ', history.history['loss'][-1])
  print('Last validation accuracy : ', history.history['val_categorical_accuracy'][-1], ' | last training accuracy : ', history.history['categorical_accuracy'][-1])
  print('Time taken in training : ', end - start, ' sec')
  return model, history.history['val_categorical_accuracy'][-1]

"""## Get best configuration from model dictionary according to highest validation accuracy"""

def get_best_configuration(models):
  maxVal = 0
  maxkey = ''
  for key,(model, value) in models.items():
    if (value > maxVal):
      maxkey = key
      maxVal = value
  return maxkey

"""## Model evaluation code"""

def evaluate_model(model, test_X, test_y):
  BATCH_SIZE = 64
  start = time.time()
  test_loss, test_acc = model.evaluate(test_X, test_y, verbose = 0, batch_size = BATCH_SIZE)
  end = time.time()
  print('Test loss:', test_loss)
  print('Test accuracy:', test_acc)
  print('Time taken in training : ', end - start, ' sec')

"""___
## Dataset setup
"""

mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = np.expand_dims(train_images, axis=-1)/255.
train_labels = np.int64(train_labels)
test_images = np.expand_dims(test_images, axis=-1)/255.
test_labels = np.int64(test_labels)
print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)
train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)
print('After one hot encoding :', train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)

plt.figure(figsize=(10,10))
random_inds = np.random.choice(60000,36)
for i in range(36):
    plt.subplot(6,6,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    image_ind = random_inds[i]
    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)
    plt.xlabel(np.where(1 == train_labels[image_ind])[0][0])

"""---
# Fully connected configuration experiments
All will use categorical crossentropy loss.
"""

fc_models = {}

"""#### Default Adam with tanh"""

fc_models['adam_tanh'] = run_experiment(build_fc_model, 'tanh', 'adam', 'categorical_crossentropy', train_images, train_labels)

"""#### Adam with learning rate 0.05 and tanh activation"""

fc_models['adam_0.005_tanh'] = run_experiment(build_fc_model, 'tanh', tf.keras.optimizers.Adam(0.005), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD and tanh activation"""

fc_models['sgd_tanh'] = run_experiment(build_fc_model, 'tanh', 'SGD', 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.001 and tanh activation"""

fc_models['sgd_0.001_tanh'] = run_experiment(build_fc_model, 'tanh', tf.keras.optimizers.SGD(0.001), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.00465 and tanh activation"""

fc_models['sgd_0.00465_tanh'] = run_experiment(build_fc_model, 'tanh', tf.keras.optimizers.SGD(0.00465), 'categorical_crossentropy', train_images, train_labels)

"""#### Adam and relu activation"""

fc_models['adam_relu'] = run_experiment(build_fc_model, 'relu', 'adam', 'categorical_crossentropy', train_images, train_labels)

"""#### SGD and relu activation"""

fc_models['sgd_0.001_relu'] = run_experiment(build_fc_model, 'relu', tf.keras.optimizers.SGD(0.001), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.0009 and relu activation"""

fc_models['sgd_0.0009_relu'] = run_experiment(build_fc_model, 'relu', tf.keras.optimizers.SGD(0.0009), 'categorical_crossentropy', train_images, train_labels)

"""# Best fully connected configuration evaluation"""

key = get_best_configuration(fc_models)
values = fc_models[key]
print(key, ' has the best validation accuracy ', values[1])
evaluate_model(values[0], test_images, test_labels)

"""---
# Convolutional configuration experiments
All will use categorical crossentropy loss.
"""

cnn_models = {}

"""#### Adam optimizer and tanh activation"""

cnn_models['adam_tanh'] = run_experiment(build_cnn_model, 'tanh', 'adam', 'categorical_crossentropy', train_images, train_labels)

"""#### Adam with learning rate 0.01 and tanh activation"""

cnn_models['adam_0.01_tanh'] = run_experiment(build_cnn_model, 'tanh', tf.keras.optimizers.Adam(0.01), 'categorical_crossentropy', train_images, train_labels)

"""#### Adam with learning rate 0.005 and tanh activation"""

cnn_models['adam_0.005_tanh'] = run_experiment(build_cnn_model, 'tanh', tf.keras.optimizers.Adam(0.005), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD and tanh activation"""

cnn_models['sgd_tanh'] = run_experiment(build_cnn_model, 'tanh', 'sgd', 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.001 and tanh activation"""

cnn_models['sgd_0.001_tanh'] = run_experiment(build_cnn_model, 'tanh', tf.keras.optimizers.SGD(0.001), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.005 and tanh activation"""

cnn_models['sgd_0.005_tanh'] = run_experiment(build_cnn_model, 'tanh', tf.keras.optimizers.SGD(0.005), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.0055 and tanh activation"""

cnn_models['sgd_0.0055_tanh'] = run_experiment(build_cnn_model, 'tanh', tf.keras.optimizers.SGD(0.0055), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD and relu activation"""

cnn_models['sgd_relu'] = run_experiment(build_cnn_model, 'relu', tf.keras.optimizers.SGD(), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.001 and relu activation"""

cnn_models['sgd_0.001_relu'] = run_experiment(build_cnn_model, 'relu', tf.keras.optimizers.SGD(0.001), 'categorical_crossentropy', train_images, train_labels)

"""#### SGD with learning rate 0.00165 and relu activation"""

cnn_models['sgd_0.00165_relu'] = run_experiment(build_cnn_model, 'relu', tf.keras.optimizers.SGD(0.00165), 'categorical_crossentropy', train_images, train_labels)

cnn_models['adam_0.0009_relu'] = run_experiment(build_cnn_model, 'relu', tf.keras.optimizers.Adam(0.0009), 'categorical_crossentropy', train_images, train_labels)

"""## CNN model evaluation"""

key = get_best_configuration(cnn_models)
values = cnn_models[key]
print(key, ' has the best validation accuracy ', values[1])
evaluate_model(values[0], test_images, test_labels)

"""# Last task"""

best_model = get_best_configuration(cnn_models)
predictions = cnn_models[best_model][0].predict(test_images)

print(predictions[0])
print('Predicted digit : ', np.where(predictions[0] == np.amax(predictions[0]))[0][0])

#TODO: identify the digit with the highest confidence prediction for the first image in the test dataset

print('Correct digit : ', np.where(1 == test_labels[0])[0][0])

